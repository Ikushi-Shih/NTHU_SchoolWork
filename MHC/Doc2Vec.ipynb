{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from math import log\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load_data...done\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "  df1 = pd.read_csv('./Data/train1.csv', header = None)\n",
    "  df2 = pd.read_csv('./Data/test1.csv', header = None)\n",
    "  df = df1.append(df2)\n",
    "  df.drop_duplicates(inplace = True)\n",
    "  df.reset_index(inplace = True, drop = True)\n",
    "  df.columns = ['peptide', 'aff', 'hla']\n",
    "  df = df.groupby(['peptide','hla']).mean().reset_index()\n",
    "  df = df.groupby('hla').filter(lambda x : len(x)>=20).reset_index(drop = True)\n",
    "  \n",
    "  df['tmp'] = df['aff'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "  alpha = df.groupby('hla').agg({'tmp':'sum'})<4\n",
    "  alpha = alpha.reset_index()\n",
    "  df = pd.merge(df, alpha, on = 'hla', how = 'left')\n",
    "  df = df[df['tmp_y'] == False]\n",
    "  df.drop(columns = ['tmp_x','tmp_y'], inplace = True)\n",
    "  \n",
    "  print('Load_data...done')\n",
    "  return df\n",
    "df = load_data()\n",
    "data = df['peptide'].apply(lambda x: ' '.join(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133268, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]\n",
    "model = Doc2Vec(documents, vector_size=80, window=4, min_count=1, workers=12)\n",
    "model.save('./Data/Doc2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Data/Doc2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = data.apply(lambda x : model.infer_vector(x))\n",
    "\n",
    "data = alpha.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "columns overlap but no suffix specified: Index([0, 1, 2, 3, 4], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-7adacd6989dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6334\u001b[0m         \u001b[0;31m# For SparseDataFrame's benefit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6335\u001b[0m         return self._join_compat(other, on=on, how=how, lsuffix=lsuffix,\n\u001b[0;32m-> 6336\u001b[0;31m                                  rsuffix=rsuffix, sort=sort)\n\u001b[0m\u001b[1;32m   6337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6338\u001b[0m     def _join_compat(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_join_compat\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort)\u001b[0m\n\u001b[1;32m   6349\u001b[0m             return merge(self, other, left_on=on, how=how,\n\u001b[1;32m   6350\u001b[0m                          \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6351\u001b[0;31m                          suffixes=(lsuffix, rsuffix), sort=sort)\n\u001b[0m\u001b[1;32m   6352\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                          validate=validate)\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         llabels, rlabels = items_overlap_with_suffix(ldata.items, lsuf,\n\u001b[0;32m--> 574\u001b[0;31m                                                      rdata.items, rsuf)\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mlindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mleft_indexer\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mleft_indexer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mitems_overlap_with_suffix\u001b[0;34m(left, lsuffix, right, rsuffix)\u001b[0m\n\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlsuffix\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5243\u001b[0m             raise ValueError('columns overlap but no suffix specified: '\n\u001b[0;32m-> 5244\u001b[0;31m                              '{rename}'.format(rename=to_rename))\n\u001b[0m\u001b[1;32m   5245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5246\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mlrenamer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: columns overlap but no suffix specified: Index([0, 1, 2, 3, 4], dtype='object')"
     ]
    }
   ],
   "source": [
    "data.join(df).join(data)da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelenc(df, enc):\n",
    "    enc = LabelEncoder().fit(df.iloc[:,0])\n",
    "    for c in df.columns:\n",
    "        df[c] = enc.transform(df[c])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tear(serie, n, col):\n",
    "    clus_df = pd.DataFrame(serie.copy())\n",
    "    for i in range(1,n+1):\n",
    "        col_name = col + '_P' + str(i)\n",
    "        clus_df[col_name] = clus_df[col].apply(lambda x : x[i-1])\n",
    "\n",
    "    cols = [c for c in clus_df.columns if c not in [col]]\n",
    "\n",
    "    return clus_df[cols]\n",
    "\n",
    "def hla_preprocess(df):\n",
    "    df['allele_type'] = df['hla'].apply(\n",
    "    lambda x: (x.startswith('DRB') and x[0:3]) or (x.startswith('HLA-DQ') and x[0:6]) or (x.startswith('HLA-DP') and x[0:6]) or x[0:3])\n",
    "\n",
    "    hla_encoder = LabelEncoder()\n",
    "    #hla_encoder.fit(df['hla'])\n",
    "    #np.save('Data/hla_encoder_classes.npy', hla_encoder.classes_)\n",
    "    hla_encoder.classes_ = np.load('./Data/hla_encoder_classes.npy')\n",
    "    df['hla'] = hla_encoder.transform(df['hla'])\n",
    "\n",
    "    allele_type_encoder = LabelEncoder()\n",
    "    #allele_type_encoder.fit(df['allele_type'])\n",
    "    #np.save('Data/allele_type_encoder_classes.npy', allele_type_encoder.classes_)\n",
    "    allele_type_encoder.classes_ = np.load('./Data/allele_type_encoder_classes.npy')\n",
    "    df['allele_type'] = allele_type_encoder.transform(df['allele_type'])\n",
    "\n",
    "    return df\n",
    "df = hla_preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = alpha.apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype float32, int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5913175837323615"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = data.join(df[['hla', 'allele_type']])\n",
    "y = df['aff'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(data, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)\n",
    "result = {}\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = MinMaxScaler()\n",
    "scl.fit(xtrain)\n",
    "xtrain_svd_scl = scl.transform(xtrain)\n",
    "xvalid_svd_scl = scl.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66946146\n",
      "Iteration 2, loss = 0.66845978\n",
      "Iteration 3, loss = 0.66798997\n",
      "Iteration 4, loss = 0.66776162\n",
      "Iteration 5, loss = 0.66788004\n",
      "Iteration 6, loss = 0.66771277\n",
      "Iteration 7, loss = 0.66750397\n",
      "Iteration 8, loss = 0.66733833\n",
      "Iteration 9, loss = 0.66721302\n",
      "Iteration 10, loss = 0.66716457\n",
      "Iteration 11, loss = 0.66717385\n",
      "Iteration 12, loss = 0.66716414\n",
      "Iteration 13, loss = 0.66713614\n",
      "Iteration 14, loss = 0.66703787\n",
      "Iteration 15, loss = 0.66705089\n",
      "Iteration 16, loss = 0.66682911\n",
      "Iteration 17, loss = 0.66688483\n",
      "Iteration 18, loss = 0.66682396\n",
      "Iteration 19, loss = 0.66669830\n",
      "Iteration 20, loss = 0.66675737\n",
      "Iteration 21, loss = 0.66657302\n",
      "Iteration 22, loss = 0.66681944\n",
      "Iteration 23, loss = 0.66682368\n",
      "Iteration 24, loss = 0.66687608\n",
      "Iteration 25, loss = 0.66680707\n",
      "Iteration 26, loss = 0.66657132\n",
      "Iteration 27, loss = 0.66653353\n",
      "Iteration 28, loss = 0.66665746\n",
      "Iteration 29, loss = 0.66646652\n",
      "Iteration 30, loss = 0.66665258\n",
      "Iteration 31, loss = 0.66673072\n",
      "Iteration 32, loss = 0.66656820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5986203232321383"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10000, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "\n",
    "mlp.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = mlp.predict_proba(xvalid_svd_scl)[:,1]\n",
    "result['mlp'] = roc_auc_score(yvalid,predictions)\n",
    "result['mlp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:   32.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   42.9s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=12)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=12)]: Done 426 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=12)]: Done 776 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=12)]: Done 1000 out of 1000 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6364097143032535"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 1000, verbose=1, n_jobs = -1)\n",
    "rf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = rf.predict_proba(xvalid_svd_scl)[:,1]\n",
    "result['RF'] = roc_auc_score(yvalid,predictions)\n",
    "result['RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    6.1s finished\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6203815395543042"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rf = RandomForestClassifier(max_depth=3, n_estimators=n_estimator)\n",
    "rf_enc = OneHotEncoder(categories='auto')\n",
    "rf_lm = LogisticRegression(C=1.0)\n",
    "rf_lm.fit(rf_enc.fit_transform(rf.apply(xtrain_svd_scl)), ytrain)\n",
    "\n",
    "predictions = rf_lm.predict_proba(rf_enc.transform(rf.apply(xvalid_svd_scl)))[:,1]\n",
    "result['RF_LR'] = roc_auc_score(yvalid,predictions)\n",
    "result['RF_LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
