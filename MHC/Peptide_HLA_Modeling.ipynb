{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import zip_longest\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from math import log\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "def read_train_test(typ, N_FOLD):\n",
    "  train = pd.read_csv('./Data/MHCPanII/{}{}.txt'.format(typ,N_FOLD), sep=\"\\t\", header=None)\n",
    "  train.columns = ['peptide', 'aff', 'hla']\n",
    "  \n",
    "  return train\n",
    "\n",
    "def slicing(string):\n",
    "  s = string\n",
    "  t = iter(s)\n",
    "  k = ','.join(a+b+c  for a,b,c in zip_longest(t, t, t, fillvalue=\"\"))\n",
    "  k = k.split(',')\n",
    "  return k\n",
    "\n",
    "def transform_hla_name(string):\n",
    "  string = string.replace('*', '')\n",
    "  string = string.replace(':', '')\n",
    "  r = re.compile(\"([a-zA-Z]+)([0-9]+)\")\n",
    "  \n",
    "  return r.match(string).group(1) + '_' + r.match(string).group(2)\n",
    "\n",
    "def hla_name_cut(string, position = 2):\n",
    "  if string.startswith('HLA'):\n",
    "    string = string.split('-')\n",
    "    r = re.compile(\"([a-zA-Z]+)([0-9]+)\")\n",
    "    string[position] = r.match(string[position]).group(1) + '_' + r.match(string[position]).group(2)\n",
    "    return string[position]\n",
    "  elif (string.startswith('DRB') & position ==1):\n",
    "    string = string.replace('_', '')\n",
    "    r = re.compile(\"([a-zA-Z]+)([0-9]+)\")\n",
    "    string = r.match(string).group(1) + '_' + r.match(string).group(2)\n",
    "    return string\n",
    "  else:\n",
    "    return np.nan\n",
    "  \n",
    "def tovec(serie, vec_size, window):\n",
    "  #serie = serie.apply(lambda x : slicing(str(x)))\n",
    "  documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(serie)]\n",
    "  model = Doc2Vec(documents, vector_size = vec_size, window = window, min_count=1, workers=12)\n",
    "  return model\n",
    "\n",
    "def rename_col(data, string):\n",
    "  name = []\n",
    "  for c in data.columns:\n",
    "    name.append(string + str(c))\n",
    "\n",
    "  return name \n",
    "\n",
    "def model2vec(train, exon_name):\n",
    "  exon = Doc2Vec.load('./Data/'+ exon_name +'_Doc2Vec')\n",
    "  \n",
    "  alpha = train[exon_name+'_x'].dropna().apply(lambda x : exon.infer_vector(x))\n",
    "  k1 = alpha.apply(pd.Series)\n",
    "  \n",
    "  alpha = train[exon_name+'_y'].dropna().apply(lambda x : exon.infer_vector(x))\n",
    "  k2 = alpha.apply(pd.Series)\n",
    "  \n",
    "  k = pd.concat([k1, k2], axis=1, sort=False)\n",
    "  k.columns = rename_col(k, exon_name+'_')\n",
    "  \n",
    "  return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/exon_Sheet1.csv', header = 1)\n",
    "df['4digit'] = df['4digit'].apply(lambda x: transform_hla_name(x))\n",
    "df.rename(columns={'4digit':'hla'},inplace = True)\n",
    "for i in range(1, 7):\n",
    "  name = 'exon'+ str(i)\n",
    "  df[name] = df[name].apply(lambda x: slicing(str(x)))\n",
    "\n",
    "train = read_train_test('train',1)\n",
    "test = read_train_test('test',1)\n",
    "\n",
    "peptide = Doc2Vec.load('./Data/Doc2Vec')\n",
    "df['hla_1'] = df['hla']; df['hla_2'] = df['hla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_test(train):\n",
    "  #split DPA/DPB DQA/DQB\n",
    "  train['hla_1'] = train['hla'].apply(lambda x: hla_name_cut(x, position = 1))\n",
    "  train['hla_2'] = train['hla'].apply(lambda x: hla_name_cut(x))\n",
    "  #merge train with exon\n",
    "  train = train[train['hla_1'].notna()]\n",
    "  train = pd.merge(train, df.drop(columns = ['8digit', 'hla_2', 'hla']), on = 'hla_1', how = 'left')\n",
    "  train = pd.merge(train, df.drop(columns = ['8digit', 'hla_1', 'hla']), on = 'hla_2', how = 'left')\n",
    "\n",
    "  alpha = train['peptide'].apply(lambda x : peptide.infer_vector(x))\n",
    "  data = alpha.apply(pd.Series)\n",
    "  data.columns = rename_col(data, 'peptide_')\n",
    "\n",
    "  exon1 = model2vec(train, 'exon1')\n",
    "  exon2 = model2vec(train, 'exon2')\n",
    "  exon3 = model2vec(train, 'exon3')\n",
    "  exon4 = model2vec(train, 'exon4')\n",
    "  exon5 = model2vec(train, 'exon5')\n",
    "  exon6 = model2vec(train, 'exon6')\n",
    "\n",
    "  y = train.aff.apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "  add = pd.concat([data,exon1,exon2,exon3,exon4,exon5,exon6], axis = 1, sort = False)\n",
    "  \n",
    "  return add, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split DPA/DPB DQA/DQB\n",
    "train['hla_1'] = train['hla'].apply(lambda x: hla_name_cut(x, position = 1))\n",
    "train['hla_2'] = train['hla'].apply(lambda x: hla_name_cut(x))\n",
    "#merge train with exon\n",
    "train = train[train['hla_1'].notna()]\n",
    "train = pd.merge(train, df.drop(columns = ['8digit', 'hla_2', 'hla']), on = 'hla_1', how = 'left')\n",
    "train = pd.merge(train, df.drop(columns = ['8digit', 'hla_1', 'hla']), on = 'hla_2', how = 'left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = train['peptide'].apply(lambda x : peptide.infer_vector(x))\n",
    "data = alpha.apply(pd.Series)\n",
    "data.columns = rename_col(data, 'peptide_')\n",
    "\n",
    "exon1 = model2vec(train, 'exon1')\n",
    "exon2 = model2vec(train, 'exon2')\n",
    "exon3 = model2vec(train, 'exon3')\n",
    "exon4 = model2vec(train, 'exon4')\n",
    "exon5 = model2vec(train, 'exon5')\n",
    "exon6 = model2vec(train, 'exon6')\n",
    "\n",
    "y = train.aff.apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "add = pd.concat([data,exon1,exon2,exon3,exon4,exon5,exon6], axis = 1, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_valid, y_valid = preprocess_train_test(test)\n",
    "add.fillna(0,inplace = True)\n",
    "add_valid.fillna(0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector:6, AUC:0.6263846753148181\n"
     ]
    }
   ],
   "source": [
    "LR = LogisticRegression(C=1.0, solver = 'liblinear')\n",
    "LR.fit(add, y)\n",
    "LR_predictions = LR.predict_proba(add_valid)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\ttraining's auc: 0.825122\tvalid_1's auc: 0.653022\n",
      "[1000]\ttraining's auc: 0.897223\tvalid_1's auc: 0.653468\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttraining's auc: 0.874565\tvalid_1's auc: 0.654003\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "MAX_BOOST_ROUNDS = 100000\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "#x_train = x_train.values.astype(np.float32, copy=False)\n",
    "d_train = lgb.Dataset(add, label= y)\n",
    "d_valid = lgb.Dataset(add_valid, label = y_valid)\n",
    "# Params\n",
    "params = {\n",
    "    'objective':'binary',\n",
    "    'metric': 'auc',\n",
    "    \"boosting\": 'gbdt', \n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'seed': 0,\n",
    "    #'is_unbalance': True,\n",
    "}\n",
    "#Model\n",
    "clf = lgb.train(\n",
    "        params=params,\n",
    "        train_set=d_train,\n",
    "        num_boost_round = MAX_BOOST_ROUNDS,\n",
    "        valid_sets=[d_train, d_valid],\n",
    "        early_stopping_rounds=200,\n",
    "        verbose_eval=500\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
