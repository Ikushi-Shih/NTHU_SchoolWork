{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from math import log, floor\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Data/preprocessed_data.csv')\n",
    "with open('./Data/feature_config.json') as json_data:\n",
    "  features = json.load(json_data)\n",
    "\n",
    "features.keys()\n",
    "\n",
    "len_columns = features['length']+ features['core_count']+ features['unique']+\\\n",
    "  features['blosum_sum']+ features['blosum']\n",
    "len_fea = df[len_columns]\n",
    "len_fea = len_fea.divide(len_fea.max())\n",
    "df.drop(columns=len_columns, inplace = True)\n",
    "df = df.join(len_fea)\n",
    "\n",
    "origin_cols = df.columns\n",
    "onehot_columns = features['hla']+features['core']+features['PFR']+features['cluster']\n",
    "one_hot = pd.get_dummies(data=df, columns=onehot_columns)\n",
    "one_hot.drop(columns = set(origin_cols)-set(onehot_columns), inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "zxc = df.copy().dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['peptide','core','LPFR','RPFR','true_index','n_label']\n",
    "data = zxc.drop(columns = cols_to_drop, axis = 1)\n",
    "y = data['aff'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "data.drop(columns = 'aff', inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(data, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)\n",
    "result = {}\n",
    "# Scale the data to (0,1) for Bayesyes\n",
    "scl = MinMaxScaler()\n",
    "scl.fit(xtrain)\n",
    "xtrain_svd_scl = scl.transform(xtrain)\n",
    "xvalid_svd_scl = scl.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hla_preprocess(df):\n",
    "  hla_encoder = LabelEncoder()\n",
    "  #hla_encoder.fit(df['hla'])\n",
    "  #np.save('Data/hla_encoder_classes.npy', hla_encoder.classes_)\n",
    "  hla_encoder.classes_ = np.load('./Data/hla_encoder_classes.npy')\n",
    "  df['hla'] = hla_encoder.transform(df['hla'])\n",
    "\n",
    "\n",
    "  return df\n",
    "\n",
    "v = pd.read_csv('./Data/vectors_data.csv')\n",
    "v = hla_preprocess(v)\n",
    "df = pd.merge(df, v, on = ['peptide', 'hla'], how = 'left')\n",
    "\n",
    "columns = features['blosum']+features['unique']+features['core_tfidf']+[c for c in list(v.columns) if c not in ['peptide', 'hla']]+['aff']\n",
    "cols_to_drop = ['aff','peptide','core','LPFR','RPFR','true_index','n_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = one_hot.join(df[columns]).drop_duplicates()\n",
    "y = data['aff'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "data.drop(columns = 'aff', inplace = True)\n",
    "data.fillna(0,inplace = True)\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(data, y, \n",
    "                                                  stratify=y, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)\n",
    "result = {}\n",
    "# Scale the data to (0,1) for Bayesyes\n",
    "scl = MinMaxScaler()\n",
    "scl.fit(xtrain)\n",
    "xtrain_svd_scl = scl.transform(xtrain)\n",
    "xvalid_svd_scl = scl.transform(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7520200975711848"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "LR = LogisticRegression(C=1.0)\n",
    "LR.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = LR.predict_proba(xvalid_svd_scl)[:,1]\n",
    "result['LR'] = roc_auc_score(yvalid,predictions)\n",
    "result['LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999991856400442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.2min finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999925058136676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999879116052635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   40.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999903263238078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   41.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999911170249711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    1.5s finished\n"
     ]
    }
   ],
   "source": [
    "X = one_hot.join(df[columns]).drop_duplicates()#.reset_index(drop = True)\n",
    "X = X.dropna().reset_index(drop = True)\n",
    "y = X['aff'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "X.drop(columns = 'aff', inplace = True)\n",
    "#X.fillna(0,inplace = True)\n",
    "\n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "\n",
    "for train_index, test_index in StratifiedKFold(n_splits=5, shuffle=True).split(X, y):\n",
    "  pred = {}\n",
    "\n",
    "  xtrain, xvalid = X.loc[train_index], X.loc[test_index]\n",
    "  ytrain, yvalid = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "  scl = MinMaxScaler()\n",
    "  scl.fit(xtrain)\n",
    "  xtrain_svd_scl = scl.transform(xtrain)\n",
    "  xvalid_svd_scl = scl.transform(xvalid)\n",
    "  \n",
    "  rf = RandomForestClassifier(n_estimators = 100, verbose=1, n_jobs = -1)\n",
    "  rf.fit(xtrain_svd_scl, ytrain)\n",
    "  predictions = rf.predict_proba(xvalid_svd_scl)[:,1]\n",
    "  \n",
    "  oof_preds[test_index] = predictions\n",
    "  \n",
    "  print(roc_auc_score(yvalid,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   54.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.6min finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  26 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=12)]: Done 100 out of 100 | elapsed:    1.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7368923008017985"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, verbose=1, n_jobs = -1)\n",
    "rf.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = rf.predict_proba(xvalid_svd_scl)[:,1]\n",
    "result['RF'] = roc_auc_score(yvalid,predictions)\n",
    "result['RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   14.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    4.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9835669217990557"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rf = RandomForestClassifier(max_depth=3, n_estimators=500)\n",
    "rf_enc = OneHotEncoder(categories='auto')\n",
    "rf_lm = LogisticRegression(C=1.0)\n",
    "rf_lm.fit(rf_enc.fit_transform(rf.apply(xtrain_svd_scl)), ytrain)\n",
    "\n",
    "predictions = rf_lm.predict_proba(rf_enc.transform(rf.apply(xvalid_svd_scl)))[:,1]\n",
    "result['RF_LR'] = roc_auc_score(yvalid,predictions)\n",
    "result['RF_LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7321672065028977"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbes = GradientBoostingClassifier(\n",
    "  n_estimators=100,\n",
    "  validation_fraction=0.2,\n",
    "  n_iter_no_change=5, tol=0.01,\n",
    "  random_state=0\n",
    ")\n",
    "gbes.fit(xtrain_svd_scl, ytrain)\n",
    "predictions = gbes.predict_proba(xvalid_svd_scl)[:,1]\n",
    "result['GB'] = roc_auc_score(yvalid,predictions)\n",
    "result['GB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3917061743e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m y_pred_grd_lm = gb_lr.predict_proba(\n\u001b[0;32m----> 6\u001b[0;31m   grd_enc.transform(grd.apply(xvalid_svd_scl)[:, :, 0]))[:, 1]\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gb_lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myvalid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred_grd_lm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gb_lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grd' is not defined"
     ]
    }
   ],
   "source": [
    "grd_enc = OneHotEncoder(categories='auto')\n",
    "gb_lr = LogisticRegression(C=1.0)\n",
    "gb_lr.fit(grd_enc.fit_transform(gbes.apply(xtrain_svd_scl)[:, :, 0]), ytrain)\n",
    "\n",
    "y_pred_grd_lm = gb_lr.predict_proba(\n",
    "  grd_enc.transform(grd.apply(xvalid_svd_scl)[:, :, 0]))[:, 1]\n",
    "result['gb_lr'] = roc_auc_score(yvalid,y_pred_grd_lm)\n",
    "result['gb_lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
