{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import * \n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, Normalizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,HashingVectorizer, TfidfTransformer\n",
    "from sklearn.cluster import KMeans, DBSCAN, AffinityPropagation\n",
    "from math import log\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelenc(df, enc):\n",
    "    for c in df.columns:\n",
    "        df[c] = enc.transform(df[c])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tear(serie, n, col):\n",
    "    clus_df = pd.DataFrame(serie.copy())\n",
    "    for i in range(1,n+1):\n",
    "        col_name = col + '_P' + str(i)\n",
    "        clus_df[col_name] = clus_df[col].apply(lambda x : x[i-1] if i<=len(x) else 'not_exist')\n",
    "\n",
    "    cols = [c for c in clus_df.columns if c not in [col]]\n",
    "\n",
    "    return clus_df[cols]\n",
    "\n",
    "def tfidf(df, col):\n",
    "    vectorizer = TfidfVectorizer(analyzer = 'char')\n",
    "    X = np.array(vectorizer.fit_transform(df[col]).toarray(), dtype=np.float16)\n",
    "\n",
    "    for i in range(len(vectorizer.get_feature_names())):\n",
    "        df[col + '_' + vectorizer.get_feature_names()[i] + '_tfidf'] = X[:, i]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def hla_preprocess(df):\n",
    "    #df['allele_type'] = df['hla'].apply(\n",
    "    #lambda x: (x.startswith('DRB') and x[0:3]) or (x.startswith('HLA-DQ') and x[0:6]) or (x.startswith('HLA-DP') and x[0:6]) or x[0:3])\n",
    "\n",
    "    hla_encoder = LabelEncoder()\n",
    "    hla_encoder.fit(df['hla'])\n",
    "    #np.save('Data/hla_encoder_classes.npy', hla_encoder.classes_)\n",
    "    df['hla'] = hla_encoder.transform(df['hla'])\n",
    "\n",
    "    #allele_type_encoder = LabelEncoder()\n",
    "    #allele_type_encoder.fit(df['allele_type'])\n",
    "    #np.save('Data/allele_type_encoder_classes.npy', allele_type_encoder.classes_)\n",
    "    #df['allele_type'] = allele_type_encoder.transform(df['allele_type'])\n",
    "\n",
    "    return df\n",
    "#df = hla_preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load_data...done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(133268, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data():\n",
    "  df1 = pd.read_csv('./Data/train1.csv', header = None)\n",
    "  df2 = pd.read_csv('./Data/test1.csv', header = None)\n",
    "  df = df1.append(df2)\n",
    "  df.drop_duplicates(inplace = True)\n",
    "  df.reset_index(inplace = True, drop = True)\n",
    "  df.columns = ['peptide', 'aff', 'hla']\n",
    "  df = df.groupby(['peptide','hla']).mean().reset_index()\n",
    "  df = df.groupby('hla').filter(lambda x : len(x)>=20).reset_index(drop = True)\n",
    "  \n",
    "  df['tmp'] = df['aff'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "  alpha = df.groupby('hla').agg({'tmp':'sum'})<4\n",
    "  alpha = alpha.reset_index()\n",
    "  df = pd.merge(df, alpha, on = 'hla', how = 'left')\n",
    "  df = df[df['tmp_y'] == False]\n",
    "  df.drop(columns = ['tmp_x','tmp_y'], inplace = True)\n",
    "  \n",
    "  print('Load_data...done')\n",
    "  return df\n",
    "df = load_data()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>peptide</th>\n",
       "      <th>hla</th>\n",
       "      <th>aff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAAAAA</td>\n",
       "      <td>H-2-IAd</td>\n",
       "      <td>0.324088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAGTTVYGAFAA</td>\n",
       "      <td>HLA-DPA10103-DPB10401</td>\n",
       "      <td>0.129502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAGTTVYGAFAA</td>\n",
       "      <td>HLA-DPA10103-DPB10601</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAGTTVYGAFAA</td>\n",
       "      <td>HLA-DQA10102-DQB10602</td>\n",
       "      <td>0.856229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAGTTVYGAFAA</td>\n",
       "      <td>HLA-DQA10401-DQB10402</td>\n",
       "      <td>0.541205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAAAAGTTVYGAFAA</td>\n",
       "      <td>HLA-DQA10501-DQB10301</td>\n",
       "      <td>0.645158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>DRB1_0101</td>\n",
       "      <td>0.239722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>DRB1_0301</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>DRB1_0401</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>DRB1_0404</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>DRB1_0405</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>DRB1_0701</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>DRB1_0802</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>HLA-DQA10301-DQB10301</td>\n",
       "      <td>0.778379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>HLA-DQA10301-DQB10302</td>\n",
       "      <td>0.505106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>HLA-DQA10501-DQB10201</td>\n",
       "      <td>0.516523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AAAAAVAAEAY</td>\n",
       "      <td>HLA-DQA10501-DQB10301</td>\n",
       "      <td>0.848308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_0101</td>\n",
       "      <td>0.803214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_0301</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_0401</td>\n",
       "      <td>0.569691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_0405</td>\n",
       "      <td>0.580092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_0701</td>\n",
       "      <td>0.583555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_1101</td>\n",
       "      <td>0.072672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_1201</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_1302</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB1_1501</td>\n",
       "      <td>0.178250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB3_0101</td>\n",
       "      <td>0.567858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB3_0202</td>\n",
       "      <td>0.393493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB4_0101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AAAAAYEAAFAATVP</td>\n",
       "      <td>DRB5_0101</td>\n",
       "      <td>0.543581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133417</th>\n",
       "      <td>YYKFLANVSTVLTGK</td>\n",
       "      <td>DRB1_0802</td>\n",
       "      <td>0.778019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133418</th>\n",
       "      <td>YYKFLANVSTVLTGK</td>\n",
       "      <td>DRB1_1001</td>\n",
       "      <td>0.922227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133419</th>\n",
       "      <td>YYKFLANVSTVLTGK</td>\n",
       "      <td>DRB1_1101</td>\n",
       "      <td>0.712727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133420</th>\n",
       "      <td>YYKFLANVSTVLTGK</td>\n",
       "      <td>DRB1_1302</td>\n",
       "      <td>0.985005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133421</th>\n",
       "      <td>YYKFLANVSTVLTGK</td>\n",
       "      <td>DRB1_1602</td>\n",
       "      <td>0.951475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133422</th>\n",
       "      <td>YYKFLANVSTVLTGK</td>\n",
       "      <td>DRB3_0202</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133423</th>\n",
       "      <td>YYNEMSRGYFEHMKKNLN</td>\n",
       "      <td>DRB1_0101</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133424</th>\n",
       "      <td>YYNLRPEVIESIYYAYRMTK</td>\n",
       "      <td>DRB1_0401</td>\n",
       "      <td>0.305876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133425</th>\n",
       "      <td>YYPTNKLQAAVMETD</td>\n",
       "      <td>H-2-IAb</td>\n",
       "      <td>0.121511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133426</th>\n",
       "      <td>YYQSGLSIVMPVGGQSSFYS</td>\n",
       "      <td>DRB1_0301</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133427</th>\n",
       "      <td>YYRYNYAFDLK</td>\n",
       "      <td>DRB1_0101</td>\n",
       "      <td>0.603461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133428</th>\n",
       "      <td>YYRYNYAFDLK</td>\n",
       "      <td>DRB1_0401</td>\n",
       "      <td>0.361562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133429</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB1_0404</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133430</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB1_0701</td>\n",
       "      <td>0.151689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133431</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB1_0801</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133432</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB1_0901</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133433</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB1_1101</td>\n",
       "      <td>0.162582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133434</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB1_1301</td>\n",
       "      <td>0.255515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133435</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB3_0101</td>\n",
       "      <td>0.335763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133436</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB3_0202</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133437</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB3_0301</td>\n",
       "      <td>0.409393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133438</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB4_0103</td>\n",
       "      <td>0.322016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133439</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>DRB5_0101</td>\n",
       "      <td>0.462363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133440</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>HLA-DQA10102-DQB10501</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133441</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>HLA-DQA10201-DQB10301</td>\n",
       "      <td>0.309946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133442</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>HLA-DQA10201-DQB10303</td>\n",
       "      <td>0.267999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133443</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>HLA-DQA10201-DQB10402</td>\n",
       "      <td>0.224732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133444</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>HLA-DQA10501-DQB10302</td>\n",
       "      <td>0.332466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133445</th>\n",
       "      <td>YYSEPTSENNAHHVC</td>\n",
       "      <td>HLA-DQA10501-DQB10402</td>\n",
       "      <td>0.351419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133446</th>\n",
       "      <td>YYSLLMPILTLTRAL</td>\n",
       "      <td>DRB1_0101</td>\n",
       "      <td>0.467327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133268 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     peptide                    hla       aff\n",
       "0                AAAAAAAAAAA                H-2-IAd  0.324088\n",
       "1            AAAAAGTTVYGAFAA  HLA-DPA10103-DPB10401  0.129502\n",
       "2            AAAAAGTTVYGAFAA  HLA-DPA10103-DPB10601  0.000000\n",
       "3            AAAAAGTTVYGAFAA  HLA-DQA10102-DQB10602  0.856229\n",
       "4            AAAAAGTTVYGAFAA  HLA-DQA10401-DQB10402  0.541205\n",
       "5            AAAAAGTTVYGAFAA  HLA-DQA10501-DQB10301  0.645158\n",
       "6                AAAAAVAAEAY              DRB1_0101  0.239722\n",
       "7                AAAAAVAAEAY              DRB1_0301  0.000000\n",
       "8                AAAAAVAAEAY              DRB1_0401  0.000000\n",
       "9                AAAAAVAAEAY              DRB1_0404  0.000000\n",
       "10               AAAAAVAAEAY              DRB1_0405  0.000000\n",
       "11               AAAAAVAAEAY              DRB1_0701  0.000000\n",
       "12               AAAAAVAAEAY              DRB1_0802  0.000000\n",
       "13               AAAAAVAAEAY  HLA-DQA10301-DQB10301  0.778379\n",
       "14               AAAAAVAAEAY  HLA-DQA10301-DQB10302  0.505106\n",
       "15               AAAAAVAAEAY  HLA-DQA10501-DQB10201  0.516523\n",
       "16               AAAAAVAAEAY  HLA-DQA10501-DQB10301  0.848308\n",
       "17           AAAAAYEAAFAATVP              DRB1_0101  0.803214\n",
       "18           AAAAAYEAAFAATVP              DRB1_0301  0.000000\n",
       "19           AAAAAYEAAFAATVP              DRB1_0401  0.569691\n",
       "20           AAAAAYEAAFAATVP              DRB1_0405  0.580092\n",
       "21           AAAAAYEAAFAATVP              DRB1_0701  0.583555\n",
       "22           AAAAAYEAAFAATVP              DRB1_1101  0.072672\n",
       "23           AAAAAYEAAFAATVP              DRB1_1201  0.000000\n",
       "24           AAAAAYEAAFAATVP              DRB1_1302  0.000000\n",
       "25           AAAAAYEAAFAATVP              DRB1_1501  0.178250\n",
       "26           AAAAAYEAAFAATVP              DRB3_0101  0.567858\n",
       "27           AAAAAYEAAFAATVP              DRB3_0202  0.393493\n",
       "28           AAAAAYEAAFAATVP              DRB4_0101  0.000000\n",
       "29           AAAAAYEAAFAATVP              DRB5_0101  0.543581\n",
       "...                      ...                    ...       ...\n",
       "133417       YYKFLANVSTVLTGK              DRB1_0802  0.778019\n",
       "133418       YYKFLANVSTVLTGK              DRB1_1001  0.922227\n",
       "133419       YYKFLANVSTVLTGK              DRB1_1101  0.712727\n",
       "133420       YYKFLANVSTVLTGK              DRB1_1302  0.985005\n",
       "133421       YYKFLANVSTVLTGK              DRB1_1602  0.951475\n",
       "133422       YYKFLANVSTVLTGK              DRB3_0202  1.000000\n",
       "133423    YYNEMSRGYFEHMKKNLN              DRB1_0101  0.000000\n",
       "133424  YYNLRPEVIESIYYAYRMTK              DRB1_0401  0.305876\n",
       "133425       YYPTNKLQAAVMETD                H-2-IAb  0.121511\n",
       "133426  YYQSGLSIVMPVGGQSSFYS              DRB1_0301  0.000000\n",
       "133427           YYRYNYAFDLK              DRB1_0101  0.603461\n",
       "133428           YYRYNYAFDLK              DRB1_0401  0.361562\n",
       "133429       YYSEPTSENNAHHVC              DRB1_0404  0.000000\n",
       "133430       YYSEPTSENNAHHVC              DRB1_0701  0.151689\n",
       "133431       YYSEPTSENNAHHVC              DRB1_0801  0.000000\n",
       "133432       YYSEPTSENNAHHVC              DRB1_0901  0.000000\n",
       "133433       YYSEPTSENNAHHVC              DRB1_1101  0.162582\n",
       "133434       YYSEPTSENNAHHVC              DRB1_1301  0.255515\n",
       "133435       YYSEPTSENNAHHVC              DRB3_0101  0.335763\n",
       "133436       YYSEPTSENNAHHVC              DRB3_0202  0.000000\n",
       "133437       YYSEPTSENNAHHVC              DRB3_0301  0.409393\n",
       "133438       YYSEPTSENNAHHVC              DRB4_0103  0.322016\n",
       "133439       YYSEPTSENNAHHVC              DRB5_0101  0.462363\n",
       "133440       YYSEPTSENNAHHVC  HLA-DQA10102-DQB10501  0.000000\n",
       "133441       YYSEPTSENNAHHVC  HLA-DQA10201-DQB10301  0.309946\n",
       "133442       YYSEPTSENNAHHVC  HLA-DQA10201-DQB10303  0.267999\n",
       "133443       YYSEPTSENNAHHVC  HLA-DQA10201-DQB10402  0.224732\n",
       "133444       YYSEPTSENNAHHVC  HLA-DQA10501-DQB10302  0.332466\n",
       "133445       YYSEPTSENNAHHVC  HLA-DQA10501-DQB10402  0.351419\n",
       "133446       YYSLLMPILTLTRAL              DRB1_0101  0.467327\n",
       "\n",
       "[133268 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_generator(string, hla, dim):\n",
    "  final = []\n",
    "  for s in string:\n",
    "    k = [0]*dim\n",
    "    k += model.wv[s]\n",
    "    k = pd.Series(np.append(k, hla))\n",
    "  #final.append(k)\n",
    "  \n",
    "  return k#final\n",
    "\n",
    "def pca_process(vectors, N_COM):\n",
    "  pca = PCA(n_components=N_COM)\n",
    "  return pd.DataFrame(pca.fit_transform(vectors))\n",
    "\n",
    "model = Word2Vec.load('./Data/HLA-Vec_Object2.model')\n",
    "df_ = df[['peptide','hla']].apply(lambda x: vector_generator(x['peptide'], x['hla'], 2), axis = 1)\n",
    "\n",
    "df_ = df_.drop(columns = 2)\n",
    "#df_ = pca_process(df_, 20)\n",
    "df_col = []\n",
    "for k in df_.columns:\n",
    "  df_col.append('peptide_vector_{}'.format(k))\n",
    "df_.columns = df_col\n",
    "df = pd.concat([df,df_], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "alpha = np.array(df['hla'])\n",
    "alpha = alpha.reshape(len(alpha), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(alpha)\n",
    "print(onehot_encoded)\n",
    "# invert first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.concat([df_, pd.DataFrame(onehot_encoded)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106677 samples, validate on 26670 samples\n",
      "Epoch 1/20\n",
      "106677/106677 [==============================] - 93s 875us/step - loss: 0.0727 - val_loss: 0.0705\n",
      "Epoch 2/20\n",
      "106677/106677 [==============================] - 94s 882us/step - loss: 0.0688 - val_loss: 0.0694\n",
      "Epoch 3/20\n",
      "106677/106677 [==============================] - 88s 822us/step - loss: 0.0679 - val_loss: 0.0711\n",
      "Epoch 4/20\n",
      "106677/106677 [==============================] - 94s 877us/step - loss: 0.0677 - val_loss: 0.0709\n",
      "Epoch 5/20\n",
      "106677/106677 [==============================] - 96s 899us/step - loss: 0.0672 - val_loss: 0.0691\n",
      "Epoch 6/20\n",
      "106677/106677 [==============================] - 94s 885us/step - loss: 0.0669 - val_loss: 0.0688\n",
      "Epoch 7/20\n",
      "106677/106677 [==============================] - 97s 910us/step - loss: 0.0667 - val_loss: 0.0692\n",
      "Epoch 8/20\n",
      "106677/106677 [==============================] - 95s 892us/step - loss: 0.0667 - val_loss: 0.0699\n",
      "Epoch 9/20\n",
      "106677/106677 [==============================] - 87s 820us/step - loss: 0.0666 - val_loss: 0.0694\n",
      "Epoch 10/20\n",
      "106677/106677 [==============================] - 92s 858us/step - loss: 0.0665 - val_loss: 0.0690\n",
      "Epoch 11/20\n",
      "106677/106677 [==============================] - 92s 858us/step - loss: 0.0664 - val_loss: 0.0691\n",
      "Epoch 12/20\n",
      "106677/106677 [==============================] - 91s 849us/step - loss: 0.0663 - val_loss: 0.0691\n",
      "Epoch 13/20\n",
      "106677/106677 [==============================] - 91s 851us/step - loss: 0.0661 - val_loss: 0.0728\n",
      "Epoch 14/20\n",
      "106677/106677 [==============================] - 90s 848us/step - loss: 0.0656 - val_loss: 0.0702\n",
      "Epoch 15/20\n",
      "106677/106677 [==============================] - 91s 849us/step - loss: 0.0651 - val_loss: 0.0687\n",
      "Epoch 16/20\n",
      "106677/106677 [==============================] - 90s 848us/step - loss: 0.0648 - val_loss: 0.0688\n",
      "Epoch 17/20\n",
      "106677/106677 [==============================] - 107s 1ms/step - loss: 0.0647 - val_loss: 0.0681\n",
      "Epoch 18/20\n",
      "106677/106677 [==============================] - 107s 1ms/step - loss: 0.0647 - val_loss: 0.0698\n",
      "Epoch 19/20\n",
      "106677/106677 [==============================] - 103s 970us/step - loss: 0.0646 - val_loss: 0.0690\n",
      "Epoch 20/20\n",
      "106677/106677 [==============================] - 104s 973us/step - loss: 0.0645 - val_loss: 0.0718\n",
      "Train on 106677 samples, validate on 26670 samples\n",
      "Epoch 1/20\n",
      "106677/106677 [==============================] - 100s 934us/step - loss: 0.0641 - val_loss: 0.0723\n",
      "Epoch 2/20\n",
      "106677/106677 [==============================] - 97s 913us/step - loss: 0.0639 - val_loss: 0.0731\n",
      "Epoch 3/20\n",
      "106677/106677 [==============================] - 97s 907us/step - loss: 0.0639 - val_loss: 0.0730\n",
      "Epoch 4/20\n",
      "106677/106677 [==============================] - 96s 902us/step - loss: 0.0637 - val_loss: 0.0730\n",
      "Epoch 5/20\n",
      "106677/106677 [==============================] - 106s 998us/step - loss: 0.0636 - val_loss: 0.0737\n",
      "Epoch 6/20\n",
      "106677/106677 [==============================] - 101s 950us/step - loss: 0.0635 - val_loss: 0.0741\n",
      "Epoch 7/20\n",
      "106677/106677 [==============================] - 112s 1ms/step - loss: 0.0635 - val_loss: 0.0748\n",
      "Epoch 8/20\n",
      "106677/106677 [==============================] - 119s 1ms/step - loss: 0.0634 - val_loss: 0.0731\n",
      "Epoch 9/20\n",
      "106677/106677 [==============================] - 115s 1ms/step - loss: 0.0633 - val_loss: 0.0739\n",
      "Epoch 10/20\n",
      "106677/106677 [==============================] - 102s 958us/step - loss: 0.0633 - val_loss: 0.0735\n",
      "Epoch 11/20\n",
      "106677/106677 [==============================] - 110s 1ms/step - loss: 0.0632 - val_loss: 0.0749\n",
      "Epoch 12/20\n",
      "106677/106677 [==============================] - 105s 989us/step - loss: 0.0632 - val_loss: 0.0730\n",
      "Epoch 13/20\n",
      "106677/106677 [==============================] - 105s 986us/step - loss: 0.0631 - val_loss: 0.0737\n",
      "Epoch 14/20\n",
      "106677/106677 [==============================] - 92s 865us/step - loss: 0.0631 - val_loss: 0.0729\n",
      "Epoch 15/20\n",
      "106677/106677 [==============================] - 92s 867us/step - loss: 0.0630 - val_loss: 0.0738\n",
      "Epoch 16/20\n",
      "106677/106677 [==============================] - 94s 878us/step - loss: 0.0630 - val_loss: 0.0750\n",
      "Epoch 17/20\n",
      "106677/106677 [==============================] - 94s 883us/step - loss: 0.0630 - val_loss: 0.0738\n",
      "Epoch 18/20\n",
      "106677/106677 [==============================] - 94s 882us/step - loss: 0.0629 - val_loss: 0.0736\n",
      "Epoch 19/20\n",
      "106677/106677 [==============================] - 96s 896us/step - loss: 0.0629 - val_loss: 0.0733\n",
      "Epoch 20/20\n",
      "106677/106677 [==============================] - 89s 833us/step - loss: 0.0629 - val_loss: 0.0751\n",
      "Train on 106678 samples, validate on 26669 samples\n",
      "Epoch 1/20\n",
      "106678/106678 [==============================] - 87s 814us/step - loss: 0.0654 - val_loss: 0.0621\n",
      "Epoch 2/20\n",
      "106678/106678 [==============================] - 86s 806us/step - loss: 0.0653 - val_loss: 0.0623\n",
      "Epoch 3/20\n",
      "106678/106678 [==============================] - 86s 803us/step - loss: 0.0652 - val_loss: 0.0615\n",
      "Epoch 4/20\n",
      "106678/106678 [==============================] - 85s 797us/step - loss: 0.0651 - val_loss: 0.0628\n",
      "Epoch 5/20\n",
      "106678/106678 [==============================] - 85s 793us/step - loss: 0.0650 - val_loss: 0.0618\n",
      "Epoch 6/20\n",
      "106678/106678 [==============================] - 84s 787us/step - loss: 0.0650 - val_loss: 0.0615\n",
      "Epoch 7/20\n",
      "106678/106678 [==============================] - 84s 785us/step - loss: 0.0650 - val_loss: 0.0626\n",
      "Epoch 8/20\n",
      "106678/106678 [==============================] - 84s 783us/step - loss: 0.0649 - val_loss: 0.0617\n",
      "Epoch 9/20\n",
      "106678/106678 [==============================] - 83s 780us/step - loss: 0.0648 - val_loss: 0.0613\n",
      "Epoch 10/20\n",
      "106678/106678 [==============================] - 83s 773us/step - loss: 0.0649 - val_loss: 0.0622\n",
      "Epoch 11/20\n",
      " 72704/106678 [===================>..........] - ETA: 25s - loss: 0.0646"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-f5c9477dc632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m   regressor.fit(X_train,y_train,batch_size=512, epochs=20, \n\u001b[0;32m---> 47\u001b[0;31m           verbose=1, validation_data=(X_test, y_test), callbacks=[earlystop])\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0moof_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# The LSTM architecture\n",
    "regressor = Sequential()\n",
    "# First LSTM layer with Dropout regularisation\n",
    "regressor.add(LSTM(units=20, return_sequences=True, input_shape=(X_train.shape[1],1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Second LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Third LSTM layer\n",
    "regressor.add(LSTM(units=50, return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "# Fourth LSTM layer\n",
    "regressor.add(LSTM(units=20))\n",
    "regressor.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressor.add(Dense(units=1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer='rmsprop',loss='mean_squared_error')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='mean_squared_error', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "X = _df.copy()\n",
    "y = df['aff']\n",
    "\n",
    "oof_preds = np.zeros(np.array(y).shape)\n",
    "for train_index, test_index in KFold(n_splits=N_FOLDS).split(X):\n",
    "    \n",
    "  X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "  y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "  X_train = np.reshape(np.array(X_train), (X_train.shape[0],X_train.shape[1],1))\n",
    "  X_test = np.reshape(np.array(X_test), (X_test.shape[0],X_test.shape[1],1))\n",
    "  \n",
    "  y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "  \n",
    "  regressor.fit(X_train,y_train,batch_size=512, epochs=20, \n",
    "          verbose=1, validation_data=(X_test, y_test), callbacks=[earlystop])\n",
    "  oof_preds[test_index] = pd.DataFrame(regressor.predict(X_test))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = tear(df['peptide'], df['peptide'].map(len).max(), 'peptide')\n",
    "#df = pd.concat([df, labelenc(a, LabelEncoder().fit(a.values.flatten()))], axis=1)\n",
    "df = tfidf(df, 'peptide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133348/133348 [35:25<00:00, 62.74it/s]   \n"
     ]
    }
   ],
   "source": [
    "with open('Data/blosum62.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "    \n",
    "output = []\n",
    "for string in tqdm(df['peptide']):\n",
    "    cnt = 1\n",
    "    tmp = 0\n",
    "    for s in string:\n",
    "        for k in string:\n",
    "            tmp += d[s][k]\n",
    "    output.append(tmp)    \n",
    "    \n",
    "df['blosum_peptide'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns='aff').to_csv('random_try.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's auc: 0.846528\tvalid_1's auc: 0.791576\n",
      "[2000]\ttraining's auc: 0.875724\tvalid_1's auc: 0.802428\n",
      "[3000]\ttraining's auc: 0.894957\tvalid_1's auc: 0.809082\n",
      "[4000]\ttraining's auc: 0.909049\tvalid_1's auc: 0.814544\n",
      "[5000]\ttraining's auc: 0.920638\tvalid_1's auc: 0.818717\n",
      "[6000]\ttraining's auc: 0.929941\tvalid_1's auc: 0.821985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2639b687aba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_valid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectors = df.columns[[c[0:len('peptide_vector_')]=='peptide_vector_' for c in df.columns]]\n",
    "cols_to_drop = ['aff_x','aff_y','peptide','core','LPFR','RPFR','oof_preds']+list(vectors)\n",
    "\n",
    "df.sample(frac=1, replace=True, random_state = 0).reset_index(drop = True)\n",
    "#df.rename(columns={'aff_x':'aff'},  inplace = True)\n",
    "X = df.drop(columns= cols_to_drop, axis = 1)\n",
    "y = df['aff_x'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0)\n",
    "\n",
    "# Parameters\n",
    "N_FOLDS = 5\n",
    "MAX_BOOST_ROUNDS = 100000\n",
    "LEARNING_RATE = 0.01\n",
    "    \n",
    "oof_preds = np.zeros(X.shape[0])\n",
    "#sub_preds = np.zeros(test_df.shape[0])\n",
    "feature_importance_df = pd.DataFrame()\n",
    "feats = [f for f in X.columns]\n",
    "\n",
    "for train_index, test_index in KFold(n_splits=N_FOLDS).split(X):\n",
    "    \n",
    "    X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "    y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "    #x_train = x_train.values.astype(np.float32, copy=False)\n",
    "    d_train = lgb.Dataset(X_train, label= y_train)\n",
    "    d_valid = lgb.Dataset(X_test, label = y_test)\n",
    "    # Params\n",
    "    params = {\n",
    "        'objective':'binary',\n",
    "        'metric': 'auc',\n",
    "        \"boosting\": 'gbdt', \n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        #'is_unbalance': True,\n",
    "    }\n",
    "    #Model\n",
    "    clf = lgb.train(\n",
    "            params=params,\n",
    "            train_set=d_train,\n",
    "            num_boost_round = MAX_BOOST_ROUNDS,\n",
    "            valid_sets=[d_train, d_valid],\n",
    "            early_stopping_rounds=200,\n",
    "            verbose_eval=1000\n",
    "        )\n",
    "    \n",
    "    oof_preds[test_index] = clf.predict(X_test)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = feats\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n",
    "    fold_importance_df[\"fold\"] = N_FOLDS + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "feature_importance_df.to_csv('stage2_FI_1.csv', index = False)\n",
    "#!cp ./feature_importance_df2.csv -d /content/../gdrive/My\\ Drive/MHC/Project/Data/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LinearSVC' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2f0afd8a0ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode_stage2.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#bst = lgb.Booster(model_file='mode.txt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#bst.predict(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearSVC' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "clf.save_model('mode_stage2.txt')\n",
    "#bst = lgb.Booster(model_file='mode.txt')\n",
    "#bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(df['aff'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0), oof_preds)\n",
    "#df.drop(columns='oof_preds',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-904db33413de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0moof_preds_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aff'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof_preds_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    269\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "f = pd.DataFrame(df['oof_preds'])\n",
    "f['oof_pred_2'] = oof_preds\n",
    "clf = SVC()\n",
    "oof_preds_2 = np.zeros(X.shape[0])\n",
    "\n",
    "for train_index, test_index in KFold(n_splits=N_FOLDS).split(f):\n",
    "  print('cnt')\n",
    "  X_train, X_test = X.loc[train_index], X.loc[test_index]\n",
    "  y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "  clf.fit(X_train, y_train) \n",
    "  oof_preds_2[test_index] = clf.predict(X_test)\n",
    "roc_auc_score(df['aff'].apply(lambda x: 1 if x >= 0.5 else 0), oof_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(df['aff'].apply(lambda x: 1 if x >= (1-log (500)/log(50000)) else 0), (f['oof_preds']+f['oof_pred_2'])/2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
