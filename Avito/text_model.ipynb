{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'boosting_type': 'dart',\n",
    "          'learning_rate': 0.01,\n",
    "          'max_bin': 15,\n",
    "          'max_depth': 17,\n",
    "          'num_leaves': 63,\n",
    "          'subsample': 0.8,\n",
    "          'subsample_freq': 5,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_lambda': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "          'objective': 'binary:logistic',\n",
    "          'eval_metric': 'auc',\n",
    "          'eta': 0.01,\n",
    "          'max_depth': 7,\n",
    "          'subsample': 0.8, \n",
    "          'colsample_bytree': 0.4,\n",
    "          'min_child_weight': 10,\n",
    "          'gamma': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Keras imports\n",
    "\n",
    "from keras.layers import Embedding, Dense, Input\n",
    "from keras.layers import Bidirectional, TimeDistributed, CuDNNGRU, CuDNNLSTM, Convolution1D\n",
    "from keras.layers import Conv1D, merge # old\n",
    "from keras.layers import Flatten, concatenate, Dropout, PReLU, Activation, BatchNormalization\n",
    "from keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D, SpatialDropout1D\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras import constraints\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU-ATT\n",
    "\n",
    "MAX_SENT_LENGTH = 50 \n",
    "MAX_SENTS = 35\n",
    "\n",
    "embedding_matrix = np.zeros((50000, 300)) # dummy\n",
    "\n",
    "# https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py AND\n",
    "# https://www.kaggle.com/hoonkeng/how-to-get-81-gru-att-lgbm-tf-idf-eda\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, use_bias=True, activation ='tanh', **kwargs):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activation\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                 shape=(input_shape[-1],1),\n",
    "                                 initializer='normal',\n",
    "                                 trainable=True)\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(name='bias', \n",
    "                                      shape=(1,),\n",
    "                                      initializer='zeros',\n",
    "                                      trainable=True)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        super(AttLayer, self).build(input_shape) \n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.dot(x, self.W)\n",
    "        if self.use_bias:\n",
    "            eij = K.bias_add(eij, self.bias)\n",
    "        if self.activation == 'tanh':\n",
    "            eij = K.tanh(eij)\n",
    "        elif self.activation =='relu':\n",
    "            eij = K.relu(eij)\n",
    "        else:\n",
    "            eij = eij\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1, keepdims=True)\n",
    "        weighted_input = x*weights\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = { 'activation': self.activation }\n",
    "        base_config = super(AttLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def get_attgru_model(gru_dense_dim = (64, 128), trainable=False, lr=0.0007, lr_decay=1e-16): \n",
    "  \n",
    "  # Encoder\n",
    "  \n",
    "    sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='main_input') \n",
    "\n",
    "    embedding_layer = Embedding(embedding_matrix.shape[0],\n",
    "                              embedding_matrix.shape[1],\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length=MAX_SENT_LENGTH,\n",
    "                              trainable=trainable)   \n",
    "\n",
    "    embedded_sequences = embedding_layer(sentence_input)\n",
    "\n",
    "    l_lstm = Bidirectional(CuDNNGRU(gru_dense_dim[0], return_sequences=True))(embedded_sequences)\n",
    "    l_dense = TimeDistributed(Dense(gru_dense_dim[1]))(l_lstm)\n",
    "    l_att = AttLayer()(l_dense)\n",
    "    sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "    # Decoder\n",
    "\n",
    "    review_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH), dtype='int32')\n",
    "    review_encoder = TimeDistributed(sentEncoder)(review_input)\n",
    "    l_lstm_sent = Bidirectional(CuDNNLSTM(gru_dense_dim[0], return_sequences=True))(review_encoder)\n",
    "    l_dense_sent = TimeDistributed(Dense(gru_dense_dim[1]))(l_lstm_sent)\n",
    "    l_att_sent = AttLayer()(l_dense_sent)\n",
    "    preds = Dense(2, activation='softmax')(l_att_sent)\n",
    "    sentDecoder = Model(review_input, preds)\n",
    "\n",
    "    sentDecoder.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizers.RMSprop(lr, lr_decay),\n",
    "                      metrics=['acc'])\n",
    "\n",
    "    return sentEncoder, sentDecoder # fit on sentDecoder\n",
    "\n",
    "sentEncoder, sentDecoder = get_attgru_model()\n",
    "\n",
    "print('encoder:')\n",
    "sentEncoder.summary()\n",
    "\n",
    "print('decorder:')\n",
    "sentDecoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi-LSTM\n",
    "\n",
    "cat_features_hash = ['cat_%d' % n for n in range(1,11)] # dummy\n",
    "max_cat_hash_size = 50000\n",
    "num_features = ['num_%d' % n for n in range(1,401)] # dummy\n",
    "MAX_WORDS = 500\n",
    "\n",
    "embedding_matrix_2 = np.zeros((50000, 200)) # dummy\n",
    "\n",
    "# https://www.kaggle.com/qinhui1999/deep-learning-is-all-you-need-lb-0-80x\n",
    "# https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "\n",
    "def get_model_bilstm(cat_embed_output_dim=10, trainable=False, gru_spec=(50, 100), gru_dropout=5e-5, lr=0.0006):\n",
    "    input_cat = Input((len(cat_features_hash), ))\n",
    "    input_num = Input((len(num_features), ))\n",
    "    input_words = Input((MAX_WORDS, ))\n",
    "\n",
    "    x_cat = Embedding(max_cat_hash_size, cat_embed_output_dim)(input_cat)    \n",
    "    x_cat = SpatialDropout1D(0.3)(x_cat)\n",
    "    x_cat = Flatten()(x_cat)\n",
    "\n",
    "    x_words = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                      weights=[embedding_matrix],\n",
    "                      trainable=trainable)(input_words)\n",
    "    x_words = SpatialDropout1D(0.25)(x_words) # 0.30\n",
    "\n",
    "    x_words = Bidirectional(CuDNNLSTM(gru_spec[0], return_sequences=True,\n",
    "                          kernel_regularizer=regularizers.l2(gru_dropout),\n",
    "                          recurrent_regularizer=regularizers.l2(gru_dropout)))(x_words)\n",
    "    x_words = Convolution1D(gru_spec[1], 3, activation=\"relu\")(x_words)\n",
    "\n",
    "    x_words1_1 = GlobalMaxPool1D()(x_words)\n",
    "    x_words1_2 = GlobalAveragePooling1D()(x_words)\n",
    "    x_words1_3 = AttentionWithContext()(x_words)\n",
    "    x_words = concatenate([x_words1_1, x_words1_2, x_words1_3])\n",
    "    x_words = Dropout(0.25)(x_words)       \n",
    "    x_words = Dense(100, activation=\"relu\")(x_words) # 100\n",
    "\n",
    "    if embedding_matrix_2 is not None:\n",
    "        x_words_2 = Embedding(embedding_matrix_2.shape[0], embedding_matrix_2.shape[1],\n",
    "                            weights=[embedding_matrix_2],\n",
    "                            trainable=trainable)(input_words)\n",
    "        x_words_2 = SpatialDropout1D(0.25)(x_words_2) # 0.30\n",
    "\n",
    "        x_words_2 = Bidirectional(CuDNNLSTM(gru_spec[0], return_sequences=True,\n",
    "                     kernel_regularizer=regularizers.l2(gru_dropout),\n",
    "                     recurrent_regularizer=regularizers.l2(gru_dropout)))(x_words_2)\n",
    "        x_words_2 = Convolution1D(gru_spec[1], 3, activation=\"relu\")(x_words_2)\n",
    "\n",
    "        x_words2_1 = GlobalMaxPool1D()(x_words_2)\n",
    "        x_words2_2 = GlobalAveragePooling1D()(x_words_2)\n",
    "        x_words2_3 = AttentionWithContext()(x_words_2)\n",
    "        x_words_2 = concatenate([x_words2_1, x_words2_2, x_words2_3])\n",
    "        x_words_2 = Dropout(0.25)(x_words_2)       \n",
    "        x_words_2 = Dense(100, activation=\"relu\")(x_words_2) # 100\n",
    "\n",
    "        x_words = concatenate([x_words, x_words_2])\n",
    "\n",
    "    # extra dense later to handle >400 numerical features\n",
    "    x_num = Dense(200, activation=\"relu\")(input_num)\n",
    "    x_num = Dropout(0.25)(x_num)\n",
    "    x_num = Dense(100, activation=\"relu\")(x_num)\n",
    "\n",
    "    x = concatenate([x_cat, x_num, x_words])\n",
    "\n",
    "    x = Dense(50 + (0 if embedding_matrix_2 is None or gru_spec[1] == 0 else 14), activation=\"relu\")(x) # was 30\n",
    "    x = Dropout(0.25)(x)\n",
    "    predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[input_cat, input_num, input_words], outputs=predictions)\n",
    "    model.compile(optimizer=optimizers.Adam(lr, decay=1e-6),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model_bilstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capsule\n",
    "# https://www.kaggle.com/chongjiujjin/capsule-net-with-gru\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "      \n",
    "def get_model_capsule(cat_embed_output_dim=10, trainable=False, gru_spec=(128, 0), gru_dropout=5e-5, lr=0.0007):\n",
    "    input_cat = Input((len(cat_features_hash), ))\n",
    "    input_num = Input((len(num_features), ))\n",
    "    input_words = Input((MAX_WORDS, ))\n",
    "\n",
    "    x_cat = Embedding(max_cat_hash_size, cat_embed_output_dim)(input_cat)    \n",
    "    x_cat = SpatialDropout1D(0.3)(x_cat)\n",
    "    x_cat = Flatten()(x_cat)\n",
    "\n",
    "    x_words = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n",
    "                      weights=[embedding_matrix],\n",
    "                      trainable=trainable)(input_words)\n",
    "    x_words = SpatialDropout1D(0.25)(x_words)\n",
    "\n",
    "    # https://github.com/mattmotoki/toxic-comment-classification/blob/master/code/modeling/Refine CapsuleNet.ipynb\n",
    "    x_words = Bidirectional(CuDNNGRU(gru_spec[0], return_sequences=True,\n",
    "                          kernel_regularizer=regularizers.l2(gru_dropout),\n",
    "                          recurrent_regularizer=regularizers.l2(gru_dropout)))(x_words)\n",
    "    x_words = PReLU()(x_words)\n",
    "    x_words = Capsule(num_capsule=10, dim_capsule=16, routings=5, share_weights=True)(x_words)\n",
    "    x_words = Flatten()(x_words)\n",
    "    x_words = Dropout(0.15)(x_words)\n",
    "\n",
    "    x_cat = Dense(100, activation=\"relu\")(x_cat)\n",
    "\n",
    "    # extra dense later to handle >400 numerical features\n",
    "    x_num = Dense(200, activation=\"relu\")(input_num)\n",
    "    x_num = Dropout(0.25)(x_num)\n",
    "    x_num = Dense(100, activation=\"relu\")(x_num)\n",
    "\n",
    "    x = concatenate([x_cat, x_num, x_words])\n",
    "\n",
    "    x = Dense(50 + (0 if embedding_matrix_2 is None or gru_spec[1] == 0 else 14), activation=\"relu\")(x) # was 30\n",
    "    x = Dropout(0.25)(x)\n",
    "    predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[input_cat, input_num, input_words], outputs=predictions)\n",
    "    model.compile(optimizer=optimizers.Adam(lr, decay=1e-6),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model_capsule()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Bi-GRU and Conv1D\n",
    "# https://www.kaggle.com/fizzbuzz/the-all-in-one-model\n",
    "\n",
    "def get_model_bigru_conv1d(cat_embed_output_dim=32, \n",
    "                           trainable=False,\n",
    "                           recurrent_units = 96,\n",
    "                           convolution_filters = 192,\n",
    "                           dense_units = [256, 128, 64],\n",
    "                           dropout_rate = 0.3,\n",
    "                           lr = 0.0006):\n",
    "  \n",
    "    input_cat = Input((len(cat_features_hash), ))\n",
    "    input_num = Input((len(num_features), ))\n",
    "    input_words = Input((MAX_WORDS, ))\n",
    "\n",
    "    x_cat = Embedding(max_cat_hash_size, cat_embed_output_dim)(input_cat)\n",
    "    x_cat = SpatialDropout1D(dropout_rate)(x_cat)\n",
    "    x_cat = Flatten()(x_cat)\n",
    "\n",
    "    x_words = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], # max_features, maxlen,\n",
    "                          weights=[embedding_matrix],\n",
    "                          trainable=trainable)(input_words)\n",
    "    x_words = SpatialDropout1D(dropout_rate)(x_words)\n",
    "\n",
    "    x_words1 = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(x_words)\n",
    "    x_words1 = Convolution1D(convolution_filters, 3, activation=\"relu\")(x_words1)\n",
    "    x_words1_1 = GlobalMaxPool1D()(x_words1)\n",
    "    x_words1_2 = GlobalAveragePooling1D()(x_words1)\n",
    "    x_words1_3 = AttentionWithContext()(x_words1)\n",
    "\n",
    "    x_words2 = Convolution1D(convolution_filters, 2, activation=\"relu\")(x_words)\n",
    "    x_words2 = Convolution1D(convolution_filters, 2, activation=\"relu\")(x_words2)\n",
    "    x_words2_1 = GlobalMaxPool1D()(x_words2)\n",
    "    x_words2_2 = GlobalAveragePooling1D()(x_words2)\n",
    "    x_words2_3 = AttentionWithContext()(x_words2)\n",
    "\n",
    "    x_num = input_num\n",
    "\n",
    "    x = concatenate([x_words1_1, x_words1_2, x_words1_3, x_words2_1, x_words2_2, x_words2_3, x_cat, x_num])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(dense_units[0], activation=\"relu\")(x)\n",
    "    x = Dense(dense_units[1], activation=\"relu\")(x)\n",
    "\n",
    "    x = concatenate([x, x_num])\n",
    "    x = Dense(dense_units[2], activation=\"relu\")(x)\n",
    "    predictions = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[input_cat, input_num, input_words], outputs=predictions)\n",
    "    model.compile(optimizer=optimizers.Adam(lr, decay=1e-6),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    return model  \n",
    "\n",
    "model = get_model_bigru_conv1d()\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
