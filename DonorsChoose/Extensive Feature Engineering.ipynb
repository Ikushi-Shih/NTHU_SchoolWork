{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Input, Embedding\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluating popularity\n",
      "end evaluating popularity\n",
      "start evaluating pos_dic\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1/6 [00:02<00:12,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:02<00:05,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:03<00:03,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:04<00:02,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:04<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6/6 [00:05<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-406c2eac56fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "stop_words = list(set(stopwords.words('english')))\n",
    "warnings.filterwarnings('ignore')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "id_column = \"id\"\n",
    "missing_token = \" UNK \"\n",
    "\n",
    "train = pd.read_csv(\"Data/train.csv\", parse_dates=[\"project_submitted_datetime\"])\n",
    "test = pd.read_csv(\"Data/test.csv\", parse_dates=[\"project_submitted_datetime\"])\n",
    "rc = pd.read_csv(\"Data/resources.csv\").fillna(missing_token)\n",
    "\n",
    "df = pd.concat([train, test], axis=0) \n",
    "\n",
    "rc['total_price'] = rc['quantity']*rc['price']\n",
    "agg_rc = rc.groupby('id').agg({'description':'count', 'quantity':'sum', 'price':'sum', 'total_price':'sum'}).rename(columns={'description':'items'})\n",
    "\n",
    "for func in ['min', 'max', 'mean','std']:\n",
    "    agg_rc_temp = rc.groupby('id').agg({'quantity':func, 'price':func, 'total_price':func}).rename(columns={'quantity':func+'_quantity', 'price':func+'_price', 'total_price':func+'_total_price'}).fillna(0)\n",
    "    agg_rc = agg_rc.join(agg_rc_temp)\n",
    "\n",
    "agg_rc = agg_rc.join(rc.groupby('id').agg({'description':lambda x:' '.join(x.values.astype(str))}).rename(columns={'description':'resource_description'}))\n",
    "\n",
    "df = df.join(agg_rc, on='id')\n",
    "#df.head(100)\n",
    "\n",
    "# extracting datetime features using datetime module \n",
    "df[\"Year\"] = df[\"project_submitted_datetime\"].dt.year\n",
    "df[\"Month\"] = df[\"project_submitted_datetime\"].dt.month\n",
    "df['Weekday'] = df['project_submitted_datetime'].dt.weekday\n",
    "df[\"Hour\"] = df[\"project_submitted_datetime\"].dt.hour\n",
    "df[\"Month_Day\"] = df['project_submitted_datetime'].dt.day\n",
    "df[\"Year_Day\"] = df['project_submitted_datetime'].dt.dayofyear\n",
    "\n",
    "#df[['Year', 'Month', 'Weekday', 'Hour', 'Month_Day', 'Year_Day']].head(10)\n",
    "\n",
    "# fillup empty values with missing token \n",
    "df['project_essay_3'] = df['project_essay_3'].fillna(missing_token)\n",
    "df['project_essay_4'] = df['project_essay_4'].fillna(missing_token)\n",
    "\n",
    "# extract length of each essay and title\n",
    "df[\"essay1_len\"] = df['project_essay_1'].apply(len)\n",
    "df[\"essay2_len\"] = df['project_essay_2'].apply(len)\n",
    "df[\"essay3_len\"] = df['project_essay_3'].apply(len)\n",
    "df[\"essay4_len\"] = df['project_essay_4'].apply(len)\n",
    "df[\"title_len\"] = df['project_title'].apply(len)\n",
    "df['resource_summary_len'] = df['project_resource_summary'].apply(len)\n",
    "df['resource_description_len'] = df['resource_description'].apply(len)\n",
    "\n",
    "df['resource_description_wc'] = df['resource_description'].apply(lambda x: len(str(x).split(' ')))\n",
    "df['title_wc'] = df['project_title'].apply(lambda x: len(str(x).split(' ')))\n",
    "df['essay1_wc'] = df['project_essay_1'].apply(lambda x: len(str(x).split(' ')))\n",
    "df['essay2_wc'] = df['project_essay_2'].apply(lambda x: len(str(x).split(' ')))\n",
    "df['essay3_wc'] = df['project_essay_3'].apply(lambda x: len(str(x).split(' ')))\n",
    "df['essay4_wc'] = df['project_essay_4'].apply(lambda x: len(str(x).split(' ')))\n",
    "df['resource_summary_wc'] = df['project_resource_summary'].apply(lambda x: len(str(x).split(' ')))\n",
    "\n",
    "#df[['essay_1_wc', 'essay_2_wc', 'essay_3_wc', 'essay_4_wc', 'title_wc','resource_summary_wc']].head(10)\n",
    "\n",
    "# combine the project essays to create a complete essay text\n",
    "df['text'] = df.apply(lambda row: ' '.join([str(row['project_essay_1']), \n",
    "                                            str(row['project_essay_2']), \n",
    "                                            str(row['project_essay_3']), \n",
    "                                            str(row['project_essay_4'])]), axis=1)\n",
    "\n",
    "# extract features from text\n",
    "df['char_count'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
    "df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \n",
    "df['title_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "df['upper_case_word_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "df['stopword_count'] = df['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\n",
    "\n",
    "#df[['char_count', 'word_count', 'word_density', 'punctuation_count', 'title_word_count', 'upper_case_word_count', 'stopword_count']].head(10)\n",
    "\n",
    "# functions to get polatiy and subjectivity of text using the module textblob\n",
    "def get_polarity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(text)\n",
    "        pol = textblob.sentiment.polarity\n",
    "    except:\n",
    "        pol = 0.0\n",
    "    return pol\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(text)\n",
    "        subj = textblob.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    return subj\n",
    "\n",
    "\n",
    "# change df_small to df to create these features on complete dataframe\n",
    "print('start evaluating popularity')\n",
    "df['polarity'] = df['text'].apply(get_polarity)\n",
    "df['subjectivity'] = df['text'].apply(get_subjectivity)\n",
    "print('end evaluating popularity')\n",
    "#df[['polarity', 'subjectivity']].head(10)\n",
    "\n",
    "pos_dic = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def pos_check(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_dic[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "# change df_small to df in all of the following lines to create features on complete data frame\n",
    "print('start evaluating pos_dic')\n",
    "df['noun_count'] = df['text'].apply(lambda x: pos_check(x, 'noun'))\n",
    "print(1)\n",
    "df['verb_count'] = df['text'].apply(lambda x: pos_check(x, 'verb'))\n",
    "print(2)\n",
    "df['adj_count'] = df['text'].apply(lambda x: pos_check(x, 'adj'))\n",
    "print(3)\n",
    "df['adv_count'] = df['text'].apply(lambda x: pos_check(x, 'adv'))\n",
    "print(4)\n",
    "df['pron_count'] = df['text'].apply(lambda x: pos_check(x, 'pron'))\n",
    "print(5)\n",
    "df.to_csv('Data/df_NLPed.csv',index = False)\n",
    "\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "print('Label Encoder...')\n",
    "cols = [\n",
    "    'teacher_id', \n",
    "    'teacher_prefix', \n",
    "    'school_state', \n",
    "    'project_grade_category', \n",
    "    'project_subject_categories', \n",
    "    'project_subject_subcategories'\n",
    "]\n",
    "\n",
    "for c in tqdm(cols):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[c].astype(str))\n",
    "    df[c] = le.transform(df[c].astype(str))\n",
    "    print(df[c].isna().any())\n",
    "del le\n",
    "gc.collect()\n",
    "print('Done.')\n",
    "\n",
    "print('Preprocessing text...')\n",
    "cols = [\n",
    "    'project_title', \n",
    "    'text', \n",
    "    'project_resource_summary',\n",
    "    'resource_description'\n",
    "]\n",
    "    \n",
    "n_features = [\n",
    "    100, \n",
    "    1000, \n",
    "    100,\n",
    "    100\n",
    "]\n",
    "\n",
    "df_idf = pd.DataFrame()\n",
    "for c_i, c in tqdm(enumerate(cols)):\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=n_features[c_i],\n",
    "        norm='l2',\n",
    "        )\n",
    "    tfidf.fit(df[c])\n",
    "    tfidf_df = np.array(tfidf.transform(df[c]).toarray(), dtype=np.float16)\n",
    "    \n",
    "    for i in range(n_features[c_i]):\n",
    "        df_idf[c + '_tfidf_' + str(i)] = tfidf_df[:, i]\n",
    "print(tfidf_df.shape)\n",
    "#del df_all\n",
    "import gc\n",
    "gc.collect()\n",
    "df_idf.to_csv('Data/df_idf.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [02:45, 41.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(317920, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing text...')\n",
    "cols = [\n",
    "    'project_title', \n",
    "    'text', \n",
    "    'project_resource_summary',\n",
    "    'resource_description'\n",
    "]\n",
    "    \n",
    "n_features = [\n",
    "    100, \n",
    "    1000, \n",
    "    100,\n",
    "    100\n",
    "]\n",
    "\n",
    "df_idf = pd.DataFrame()\n",
    "for c_i, c in tqdm(enumerate(cols)):\n",
    "    df[c] = df[c].astype(str)\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=n_features[c_i],\n",
    "        norm='l2',\n",
    "        )\n",
    "    tfidf.fit(df[c])\n",
    "    tfidf_df = np.array(tfidf.transform(df[c]).toarray(), dtype=np.float16)\n",
    "    \n",
    "    for i in range(n_features[c_i]):\n",
    "        df_idf[c + '_tfidf_' + str(i)] = tfidf_df[:, i]\n",
    "print(tfidf_df.shape)\n",
    "#del df_all\n",
    "import gc\n",
    "gc.collect()\n",
    "df_idf.to_csv('Data/df_idf_2.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/df_NLPed.csv')\n",
    "df_idf = pd.read_csv('Data/df_idf_2.csv')\n",
    "\n",
    "df_idf.index = range(0,len(df_idf))\n",
    "df.index = range(0,len(df))\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|█▋        | 1/6 [00:03<00:19,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 2/6 [00:04<00:09,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 3/6 [00:06<00:06,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 4/6 [00:07<00:03,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|████████▎ | 5/6 [00:08<00:01,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 6/6 [00:09<00:00,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Label Encoder...')\n",
    "cols = [\n",
    "    'teacher_id', \n",
    "    'teacher_prefix', \n",
    "    'school_state', \n",
    "    'project_grade_category', \n",
    "    'project_subject_categories', \n",
    "    'project_subject_subcategories'\n",
    "]\n",
    "\n",
    "for c in tqdm(cols):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df[c].astype(str))\n",
    "    df[c] = le.transform(df[c].astype(str))\n",
    "    print(df[c].isna().any())\n",
    "del le\n",
    "gc.collect()\n",
    "print('Done.')\n",
    "\n",
    "data = pd.concat((df,df_idf),axis=1)\n",
    "data.shape + df.shape + df_idf.shape\n",
    "\n",
    "\n",
    "final_test = data[data.project_is_approved.isnull()]\n",
    "final_test = final_test.drop_duplicates('id')\n",
    "test = test.drop(columns=['teacher_id', 'teacher_prefix', 'school_state',\n",
    "       'project_submitted_datetime', 'project_grade_category',\n",
    "       'project_subject_categories', 'project_subject_subcategories',\n",
    "       'project_title', 'project_essay_1', 'project_essay_2',\n",
    "       'project_essay_3', 'project_essay_4', 'project_resource_summary',\n",
    "       'teacher_number_of_previously_posted_projects'])\n",
    "final_test = pd.merge(test,final_test,on = ['id'], how = 'left')\n",
    "train = data[data.project_is_approved.notnull()]\n",
    "\n",
    "#del data\n",
    "#gc.collect()\n",
    "\n",
    "to_drop = ['id','project_essay_1','project_essay_2','project_essay_3','project_essay_4',\n",
    "           'project_resource_summary','project_submitted_datetime','project_title','resource_description',\n",
    "          'text','project_is_approved']\n",
    "\n",
    "final_test = final_test.drop(columns=to_drop)\n",
    "x = train.drop(columns=to_drop)\n",
    "y = train.project_is_approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train.drop(columns=to_drop)\n",
    "y = train.project_is_approved\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/healer/src/tqdm/tqdm/_tqdm.py\", line 148, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/local/Cellar/python/3.6.4_4/Frameworks/Python.framework/Versions/3.6/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's auc: 0.744844\n",
      "[400]\tvalid_0's auc: 0.758327\n",
      "[600]\tvalid_0's auc: 0.765869\n",
      "[800]\tvalid_0's auc: 0.770957\n",
      "[1000]\tvalid_0's auc: 0.775192\n",
      "[1200]\tvalid_0's auc: 0.777246\n",
      "[1400]\tvalid_0's auc: 0.778957\n",
      "[1600]\tvalid_0's auc: 0.779077\n",
      "[1800]\tvalid_0's auc: 0.779658\n",
      "[2000]\tvalid_0's auc: 0.780452\n",
      "[2200]\tvalid_0's auc: 0.781094\n",
      "[2400]\tvalid_0's auc: 0.780824\n",
      "Early stopping, best iteration is:\n",
      "[2208]\tvalid_0's auc: 0.781152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [29:03, 1743.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's auc: 0.744155\n",
      "[400]\tvalid_0's auc: 0.757485\n",
      "[600]\tvalid_0's auc: 0.765087\n",
      "[800]\tvalid_0's auc: 0.771507\n",
      "[1000]\tvalid_0's auc: 0.775316\n",
      "[1200]\tvalid_0's auc: 0.777158\n",
      "[1400]\tvalid_0's auc: 0.778707\n",
      "[1600]\tvalid_0's auc: 0.779001\n",
      "[1800]\tvalid_0's auc: 0.779758\n",
      "[2000]\tvalid_0's auc: 0.780181\n",
      "[2200]\tvalid_0's auc: 0.780692\n",
      "Early stopping, best iteration is:\n",
      "[2187]\tvalid_0's auc: 0.780708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [57:04, 1712.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's auc: 0.684961\n",
      "[400]\tvalid_0's auc: 0.69658\n",
      "[600]\tvalid_0's auc: 0.721621\n",
      "[800]\tvalid_0's auc: 0.725249\n",
      "Early stopping, best iteration is:\n",
      "[713]\tvalid_0's auc: 0.728929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [1:06:12, 1324.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's auc: 0.979123\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's auc: 0.980704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [1:08:32, 1028.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\tvalid_0's auc: 0.638229\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's auc: 0.741756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [1:10:46, 849.25s/it] \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "params = {\n",
    "        'boosting_type': 'dart',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.25,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'seed': 0,\n",
    "        'verbose': 0,\n",
    "        }\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "p_buf = []\n",
    "for train_index, val_index in tqdm(skf.split(x, y)):\n",
    "    x_train, x_test = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_test = y[train_index], y[val_index]\n",
    "    model = lgb.train(\n",
    "                params,\n",
    "                lgb.Dataset(x_train, y_train),\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=[lgb.Dataset(x_test, y_test)],\n",
    "                early_stopping_rounds=200,\n",
    "                verbose_eval=200)\n",
    "    p = model.predict(final_test, num_iteration=model.best_iteration)\n",
    "    if len(p_buf) == 0:\n",
    "        p_buf = np.array(p, dtype=np.float16)\n",
    "    else:\n",
    "        p_buf += np.array(p, dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.feature_importance()\n",
    "model_fnames = model.feature_name()\n",
    "tuples = np.array(sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = tuples[145:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "pred_lgb = model.predict(x_test)\n",
    "fi = model.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'teacher_id', 'teacher_prefix', 'school_state',\n",
       "       'project_submitted_datetime', 'project_grade_category',\n",
       "       'project_subject_categories', 'project_subject_subcategories',\n",
       "       'project_title', 'project_essay_1', 'project_essay_2',\n",
       "       'project_essay_3', 'project_essay_4', 'project_resource_summary',\n",
       "       'teacher_number_of_previously_posted_projects'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78035, 1357)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3232"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p_buf/5).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = model.predict(final_test, num_iteration=model.best_iteration)\n",
    "d = {'id':test.id , 'project_is_approved':p_buf/5}\n",
    "submission = pd.DataFrame(data = d)\n",
    "submission.to_csv('0419.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['teacher_number_of_previously_posted_projects', '165'],\n",
       "       ['items', '161'],\n",
       "       ['std_price', '134'],\n",
       "       ['noun_count', '103'],\n",
       "       ['essay2_wc', '99'],\n",
       "       ['text_tfidf_118', '88'],\n",
       "       ['text_tfidf_551', '61'],\n",
       "       ['project_title_tfidf_10', '59'],\n",
       "       ['text_tfidf_981', '56'],\n",
       "       ['price', '49'],\n",
       "       ['resource_description_tfidf_85', '45'],\n",
       "       ['project_resource_summary_tfidf_8', '43'],\n",
       "       ['resource_description_tfidf_7', '42'],\n",
       "       ['text_tfidf_87', '42'],\n",
       "       ['text_tfidf_652', '35'],\n",
       "       ['resource_description_wc', '32'],\n",
       "       ['resource_description_tfidf_14', '31'],\n",
       "       ['text_tfidf_939', '30'],\n",
       "       ['text_tfidf_976', '28'],\n",
       "       ['text_tfidf_661', '24'],\n",
       "       ['essay2_len', '21'],\n",
       "       ['resource_description_len', '20'],\n",
       "       ['essay3_len', '19'],\n",
       "       ['resource_description_tfidf_60', '18'],\n",
       "       ['resource_description_tfidf_19', '18'],\n",
       "       ['project_resource_summary_tfidf_51', '18'],\n",
       "       ['text_tfidf_553', '16'],\n",
       "       ['std_total_price', '16'],\n",
       "       ['project_subject_categories', '16'],\n",
       "       ['resource_description_tfidf_65', '15'],\n",
       "       ['resource_description_tfidf_46', '15'],\n",
       "       ['text_tfidf_515', '14'],\n",
       "       ['essay3_wc', '14'],\n",
       "       ['project_resource_summary_tfidf_78', '12'],\n",
       "       ['project_resource_summary_tfidf_47', '12'],\n",
       "       ['text_tfidf_835', '12'],\n",
       "       ['text_tfidf_114', '12'],\n",
       "       ['resource_description_tfidf_22', '11'],\n",
       "       ['text_tfidf_469', '11'],\n",
       "       ['resource_description_tfidf_20', '10'],\n",
       "       ['project_resource_summary_tfidf_16', '10'],\n",
       "       ['resource_description_tfidf_98', '9'],\n",
       "       ['resource_description_tfidf_74', '8'],\n",
       "       ['resource_description_tfidf_70', '8'],\n",
       "       ['text_tfidf_144', '8'],\n",
       "       ['std_quantity', '8'],\n",
       "       ['resource_description_tfidf_75', '7'],\n",
       "       ['essay4_wc', '7'],\n",
       "       ['min_total_price', '7'],\n",
       "       ['resource_description_tfidf_61', '6'],\n",
       "       ['resource_description_tfidf_38', '6'],\n",
       "       ['project_resource_summary_tfidf_0', '6'],\n",
       "       ['text_tfidf_909', '6'],\n",
       "       ['text_tfidf_691', '6'],\n",
       "       ['text_tfidf_470', '6'],\n",
       "       ['text_tfidf_86', '6'],\n",
       "       ['text_tfidf_39', '6'],\n",
       "       ['project_title_tfidf_55', '6'],\n",
       "       ['essay4_len', '6'],\n",
       "       ['Year_Day', '6'],\n",
       "       ['total_price', '6'],\n",
       "       ['project_title_tfidf_78', '5'],\n",
       "       ['resource_description_tfidf_86', '4'],\n",
       "       ['resource_description_tfidf_3', '4'],\n",
       "       ['project_resource_summary_tfidf_4', '4'],\n",
       "       ['text_tfidf_940', '4'],\n",
       "       ['text_tfidf_721', '4'],\n",
       "       ['text_tfidf_364', '4'],\n",
       "       ['max_price', '4'],\n",
       "       ['quantity', '4'],\n",
       "       ['resource_description_tfidf_80', '3'],\n",
       "       ['resource_description_tfidf_48', '3'],\n",
       "       ['project_resource_summary_tfidf_34', '3'],\n",
       "       ['text_tfidf_883', '3'],\n",
       "       ['text_tfidf_857', '3'],\n",
       "       ['upper_case_word_count', '3'],\n",
       "       ['punctuation_count', '3'],\n",
       "       ['word_density', '3'],\n",
       "       ['word_count', '3'],\n",
       "       ['essay1_wc', '3'],\n",
       "       ['resource_summary_len', '3'],\n",
       "       ['essay1_len', '3'],\n",
       "       ['resource_description_tfidf_64', '2'],\n",
       "       ['project_resource_summary_tfidf_80', '2'],\n",
       "       ['text_tfidf_891', '2'],\n",
       "       ['text_tfidf_862', '2'],\n",
       "       ['text_tfidf_724', '2'],\n",
       "       ['text_tfidf_617', '2'],\n",
       "       ['text_tfidf_404', '2'],\n",
       "       ['text_tfidf_312', '2'],\n",
       "       ['text_tfidf_265', '2'],\n",
       "       ['text_tfidf_158', '2'],\n",
       "       ['text_tfidf_117', '2'],\n",
       "       ['Month', '2'],\n",
       "       ['min_quantity', '2'],\n",
       "       ['resource_description_tfidf_97', '1'],\n",
       "       ['resource_description_tfidf_91', '1'],\n",
       "       ['resource_description_tfidf_88', '1'],\n",
       "       ['resource_description_tfidf_76', '1'],\n",
       "       ['resource_description_tfidf_73', '1'],\n",
       "       ['resource_description_tfidf_57', '1'],\n",
       "       ['resource_description_tfidf_55', '1'],\n",
       "       ['resource_description_tfidf_37', '1'],\n",
       "       ['resource_description_tfidf_34', '1'],\n",
       "       ['resource_description_tfidf_6', '1'],\n",
       "       ['resource_description_tfidf_1', '1'],\n",
       "       ['resource_description_tfidf_0', '1'],\n",
       "       ['project_resource_summary_tfidf_82', '1'],\n",
       "       ['project_resource_summary_tfidf_57', '1'],\n",
       "       ['project_resource_summary_tfidf_55', '1'],\n",
       "       ['project_resource_summary_tfidf_38', '1'],\n",
       "       ['project_resource_summary_tfidf_24', '1'],\n",
       "       ['text_tfidf_989', '1'],\n",
       "       ['text_tfidf_954', '1'],\n",
       "       ['text_tfidf_935', '1'],\n",
       "       ['text_tfidf_906', '1'],\n",
       "       ['text_tfidf_872', '1'],\n",
       "       ['text_tfidf_856', '1'],\n",
       "       ['text_tfidf_838', '1'],\n",
       "       ['text_tfidf_836', '1'],\n",
       "       ['text_tfidf_803', '1'],\n",
       "       ['text_tfidf_637', '1'],\n",
       "       ['text_tfidf_593', '1'],\n",
       "       ['text_tfidf_585', '1'],\n",
       "       ['text_tfidf_488', '1'],\n",
       "       ['text_tfidf_471', '1'],\n",
       "       ['text_tfidf_439', '1'],\n",
       "       ['text_tfidf_436', '1'],\n",
       "       ['text_tfidf_374', '1'],\n",
       "       ['text_tfidf_240', '1'],\n",
       "       ['text_tfidf_204', '1'],\n",
       "       ['text_tfidf_140', '1'],\n",
       "       ['text_tfidf_109', '1'],\n",
       "       ['text_tfidf_105', '1'],\n",
       "       ['text_tfidf_54', '1'],\n",
       "       ['project_title_tfidf_83', '1'],\n",
       "       ['project_title_tfidf_25', '1'],\n",
       "       ['project_title_tfidf_24', '1'],\n",
       "       ['min_price', '1'],\n",
       "       ['adv_count', '1'],\n",
       "       ['polarity', '1'],\n",
       "       ['title_word_count', '1'],\n",
       "       ['char_count', '1'],\n",
       "       ['Weekday', '1'],\n",
       "       ['Year', '1']], dtype='<U44')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples[0:145]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=2.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from tqdm import tqdm\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "train_index, val_index = skf.split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from tqdm import tqdm\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "for train_index, val_index in tqdm(skf.split(x, y)):\n",
    "    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    feature_importance, pred_lgb = gbm(x_train, y_train, x_val, y_val)\n",
    "    roc_curve(y_val, pred_lgb)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "769px",
    "left": "839px",
    "right": "64px",
    "top": "0px",
    "width": "537px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
